Tr√®s bien‚ÄØ! Voici une version **recr√©√©e et structur√©e proprement** de votre `README.md`, incluant un sch√©ma Mermaid, des sections claires et professionnelles, tout en conservant l‚Äôesprit de votre message original.

---

# ü©∫ Pipeline de Traitement de Contenu M√©dical

Ce projet met en ≈ìuvre un pipeline distribu√© pour ing√©rer, enrichir, stocker et interroger du contenu m√©dical. Il s'appuie sur une architecture microservices, Kafka pour la communication asynchrone, MongoDB pour le stockage et LM Studio pour l‚Äôenrichissement par IA.

---

## üìë Table des Mati√®res

* [üöÄ Aper√ßu du Projet](#-aper√ßu-du-projet)
* [‚ú® Fonctionnalit√©s Cl√©s](#-fonctionnalit√©s-cl√©s)
* [üß± Architecture Globale](#-architecture-globale)

  * [üìä Diagramme Mermaid](#-diagramme-mermaid)
  * [üîÅ D√©tail du Flux de Donn√©es](#-d√©tail-du-flux-de-donn√©es)
* [üõ†Ô∏è Technologies Utilis√©es](#-technologies-utilis√©es)
* [‚öôÔ∏è Pr√©requis](#-pr√©requis)
* [üì¶ Installation](#-installation)
* [‚ñ∂Ô∏è Lancement des Services](#Ô∏è-lancement-des-services)
* [üîó Points d'Acc√®s API](#-points-dacc√®s-api)
* [üìÅ Structure du Projet](#-structure-du-projet)
* [üß© Am√©liorations Possibles](#-am√©liorations-possibles)

---

## üöÄ Aper√ßu du Projet

L'objectif est de construire un syst√®me **scalable** et **modulaire** pour traiter du texte m√©dical. Les messages sont transmis entre microservices via **Apache Kafka**, enrichis par un mod√®le de langage (via **LM Studio**), stock√©s dans **MongoDB** et consultables via des API **REST** et **GraphQL**.

---

## ‚ú® Fonctionnalit√©s Cl√©s

‚úÖ Ingestion via API REST ou gRPC

‚úÖ Traitement asynchrone et d√©coupl√© via Kafka

‚úÖ Classification automatique par mots-cl√©s

‚úÖ Enrichissement intelligent via LM Studio

‚úÖ Stockage durable dans MongoDB

‚úÖ Acc√®s flexible via GraphQL ou REST


---

## üß± Architecture Globale



```mermaid
graph TD
    %% Clients externes
    ClientREST[Client Ingestion REST]
    ClientGRPC[Client Ingestion gRPC]
    ClientGraphQL[Client Requ√™te GraphQL]

    %% Points d'entr√©e & requ√™tes
    APIGW["API Gateway<br>(localhost:8000)<br>Express, Apollo Server"]
    GRPCReceiver["Content Receiver<br>(localhost:50051)<br>gRPC Server"]
    GraphQLQuery["Query Service<br>(localhost:4000)<br>Apollo Server"]

    %% Kafka Topics
    TopicIncoming["Topic:<br>incoming-medical-content"]
    TopicClassified["Topic:<br>classified-content"]

    %% Traitement
    Classifier["Classifier Service<br>Kafka Consumer/Producer"]
    LMStudio[(LM Studio Externe)]

    %% Stockage
    Storage["Storage Service<br>(localhost:8001)<br>Express, Mongoose, Kafka Consumer"]
    Mongo[(MongoDB)]

    %% Ingestion Flow
    ClientREST -->|"POST /api/content"| APIGW
    APIGW -->|"Produit msg"| TopicIncoming

    ClientGRPC -->|"ProcessContent"| GRPCReceiver
    GRPCReceiver -->|"Produit msg"| TopicIncoming

    %% Traitement Flow
    Classifier -->|"Consomme"| TopicIncoming
    Classifier -->|"Appel HTTP POST"| LMStudio
    LMStudio -->|"Texte enrichi"| Classifier
    Classifier -->|"Produit msg classifi√©"| TopicClassified

    %% Stockage Flow
    Storage -->|"Consomme"| TopicClassified
    Storage -->|"Stocke/Lit"| Mongo

    %% Requ√™te Flow
    ClientGraphQL -->|"Requ√™te GraphQL /graphql"| APIGW
    APIGW -->|"Resolver appelle GET /api/contents"| Storage

    ClientGraphQL -->|"Requ√™te GraphQL /graphql"| GraphQLQuery
    GraphQLQuery -->|"Resolver appelle GET /api/contents"| Storage

    %% Classes
    classDef service fill:#D6EAF8,stroke:#2980B9,stroke-width:2px,color:#000;
    class APIGW,GRPCReceiver,GraphQLQuery,Classifier,Storage service;

    classDef topic fill:#FDEBD0,stroke:#E67E22,stroke-width:2px,color:#000;
    class TopicIncoming,TopicClassified topic;

    classDef db fill:#D5F5E3,stroke:#27AE60,stroke-width:2px,color:#000;
    class Mongo db;

    classDef external fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000;
    class LMStudio external;

    classDef client fill:#EAEDED,stroke:#7F8C8D,stroke-width:2px,color:#000;
    class ClientREST,ClientGRPC,ClientGraphQL client;

```

---

## üîÅ D√©tail du Flux de Donn√©es

1. **Ingestion**

   * Un client envoie une requ√™te `POST /api/content` (REST) ou appelle `ProcessContent` (gRPC).
   * Le message est publi√© sur `incoming-medical-content`.

2. **Classification & Enrichissement**

   * Le `Classifier Service` consomme les messages.
   * Classification simple bas√©e sur mots-cl√©s.
   * Envoi du texte √† **LM Studio** pour r√©sum√©/analyse.
   * R√©sultat enrichi publi√© sur `classified-content`.

3. **Stockage**

   * Le `Storage Service` consomme le topic `classified-content`.
   * Les donn√©es sont enregistr√©es dans **MongoDB**.
   * Une API REST (`GET /api/contents`) permet la r√©cup√©ration.

4. **Consultation**

   * Via API Gateway (GraphQL) ou `Query Service`, les clients peuvent interroger les contenus.
   * Les resolvers appellent l‚ÄôAPI REST du `Storage Service`.

---
# ... (Toutes les sections pr√©c√©dentes du README) ...

## Tests et D√©monstration du Fonctionnement

Cette section d√©crit comment tester les diff√©rentes parties du pipeline et illustre le fonctionnement attendu. Assurez-vous que tous les services (Kafka, MongoDB, LM Studio, et vos microservices) sont d√©marr√©s comme d√©crit dans la section [Lancement des Services](#lancement-des-services).

### Outils Recommand√©s pour les Tests

*   **Postman / Insomnia / `curl` :** Pour tester les endpoints REST et GraphQL.
*   **Client gRPC (BloomRPC, grpcurl, Postman gRPC) :** Pour tester le `Content Receiver Service`.
*   **Outil de gestion Kafka (Kafka Tool, Offset Explorer, kcat/kafkacat) :** Pour inspecter les messages dans les topics Kafka.
*   **Client MongoDB (MongoDB Compass, `mongosh`) :** Pour v√©rifier les donn√©es dans la base de donn√©es.
*   **Apollo Sandbox / GraphQL Playground :** Int√©gr√© avec Apollo Server (accessible via `http://localhost:8000/graphql` et `http://localhost:4000/graphql` si activ√©) pour tester les requ√™tes GraphQL.

---

### Flux 1: Ingestion via API Gateway (REST) et Traitement Complet

1.  **√âtape 1: Soumission du Contenu via API Gateway (REST)**
    *   **Action :** Envoyez une requ√™te `POST` √† `http://localhost:8000/api/content` avec un corps JSON.
        ```json
        {
          "text": "Le patient pr√©sente des sympt√¥mes de grippe : fi√®vre √©lev√©e et toux s√®che. Ant√©c√©dents de cardiopathie.",
          "metadata": { "patient_id": "P001", "source": "Consultation Dr. Bernard" }
        }
        ```
    *   **Observations attendues :**
        *   L'API Gateway r√©pond avec un statut `200 OK` et `{ "message": "Content received", "status": "SUCCESS" }`.
        *   Logs de l'API Gateway indiquant la r√©ception et la production du message vers Kafka.
    *   **Capture d'√©cran sugg√©r√©e (api_gateway_post_request.png) :**
        *   Postman (ou √©quivalent) montrant la requ√™te envoy√©e et la r√©ponse re√ßue.
        ```
        <!-- Ins√©rez ici une capture d'√©cran de Postman montrant la requ√™te POST et la r√©ponse -->
        ```

2.  **√âtape 2: V√©rification du Message sur le Topic `incoming-medical-content`**
    *   **Action :** Utilisez un outil Kafka pour inspecter le topic `incoming-medical-content`.
    *   **Observations attendues :**
        *   Un nouveau message appara√Æt sur le topic, contenant le `text` et les `metadata` soumis, ainsi que les `headers` (`source: api-gateway`, `timestamp`).
    *   **Capture d'√©cran sugg√©r√©e (kafka_topic_incoming.png) :**
        *   Votre outil Kafka montrant le message sur le topic `incoming-medical-content`.
        ```
        <!-- Ins√©rez ici une capture d'√©cran de l'outil Kafka montrant le message sur incoming-medical-content -->
        ```

3.  **√âtape 3: Traitement par le Classifier Service et LM Studio**
    *   **Action :** Observez les logs du `Classifier Service` et, si possible, de LM Studio.
    *   **Observations attendues (Classifier Service Logs) :**
        *   Log indiquant la r√©ception du message depuis `incoming-medical-content`.
        *   Log indiquant la cat√©gorie d√©termin√©e (ex: `general` ou `cardiology` si un mot-cl√© est trouv√©).
        *   Log indiquant l'appel √† LM Studio.
        *   Log indiquant la r√©ponse re√ßue de LM Studio (l'enrichissement).
        *   Log indiquant la production du message enrichi vers `classified-content`.
    *   **Observations attendues (LM Studio Logs - si visible) :**
        *   Log d'une requ√™te entrante avec le texte √† analyser.
    *   **Capture d'√©cran sugg√©r√©e (classifier_service_logs.png) :**
        *   Terminal montrant les logs pertinents du `Classifier Service`.
        ```
        <!-- Ins√©rez ici une capture d'√©cran des logs du Classifier Service -->
        ```

4.  **√âtape 4: V√©rification du Message sur le Topic `classified-content`**
    *   **Action :** Utilisez un outil Kafka pour inspecter le topic `classified-content`.
    *   **Observations attendues :**
        *   Un nouveau message appara√Æt sur le topic, contenant les donn√©es originales, `category`, `lm_enhancement`, et `classifiedAt`.
    *   **Capture d'√©cran sugg√©r√©e (kafka_topic_classified.png) :**
        *   Votre outil Kafka montrant le message enrichi sur le topic `classified-content`.
        ```
        <!-- Ins√©rez ici une capture d'√©cran de l'outil Kafka montrant le message sur classified-content -->
        ```

5.  **√âtape 5: Stockage par le Storage Service**
    *   **Action :** Observez les logs du `Storage Service`.
    *   **Observations attendues (Storage Service Logs) :**
        *   Log indiquant la r√©ception du message depuis `classified-content`.
        *   Log indiquant que le contenu a √©t√© stock√© dans MongoDB.
    *   **Capture d'√©cran sugg√©r√©e (storage_service_logs.png) :**
        *   Terminal montrant les logs pertinents du `Storage Service`.
        ```
        <!-- Ins√©rez ici une capture d'√©cran des logs du Storage Service -->
        ```

6.  **√âtape 6: V√©rification des Donn√©es dans MongoDB**
    *   **Action :** Utilisez un client MongoDB pour interroger la collection `contents` dans la base `medical`.
    *   **Observations attendues :**
        *   Un nouveau document existe avec les donn√©es correspondantes (texte, m√©tadonn√©es, cat√©gorie, enrichissement LM).
    *   **Capture d'√©cran sugg√©r√©e (mongodb_data.png) :**
        *   MongoDB Compass (ou √©quivalent) montrant le document ins√©r√©.
        ```
        <!-- Ins√©rez ici une capture d'√©cran de MongoDB Compass montrant le nouveau document -->
        ```

7.  **√âtape 7: Consultation via l'API REST du Storage Service**
    *   **Action :** Envoyez une requ√™te `GET` √† `http://localhost:8001/api/contents`.
    *   **Observations attendues :**
        *   Une r√©ponse JSON contenant un tableau d'objets, incluant le nouveau contenu stock√©.
    *   **Capture d'√©cran sugg√©r√©e (storage_service_get_api.png) :**
        *   Postman (ou navigateur) montrant la r√©ponse de l'API.
        ```
        <!-- Ins√©rez ici une capture d'√©cran de la r√©ponse GET /api/contents -->
        ```

8.  **√âtape 8: Consultation via GraphQL (API Gateway ou Query Service)**
    *   **Action :** Utilisez Apollo Sandbox/Playground (ou Postman) pour envoyer une requ√™te GraphQL √† `http://localhost:8000/graphql` (API Gateway) ou `http://localhost:4000/graphql` (Query Service).
        ```graphql
        query {
          contents {
            id
            text
            category
            lm_enhancement
            metadata
          }
        }
        ```
    *   **Observations attendues :**
        *   Une r√©ponse JSON GraphQL contenant les donn√©es du contenu soumis.
    *   **Capture d'√©cran sugg√©r√©e (graphql_query_result.png) :**
        *   Apollo Sandbox/Playground montrant la requ√™te et la r√©ponse GraphQL.
        ```
        <!-- Ins√©rez ici une capture d'√©cran d'Apollo Sandbox/Playground avec la requ√™te et la r√©ponse -->
        ```

---

### Flux 2: Ingestion via Content Receiver Service (gRPC)

1.  **√âtape 1: Soumission du Contenu via gRPC**
    *   **Action :** Utilisez un client gRPC pour appeler la m√©thode `ProcessContent` du service `ContentReceiver` sur `localhost:50051`.
        *   **Requ√™te :**
            ```json // Repr√©sentation JSON de la requ√™te gRPC
            {
              "text": "√âruption cutan√©e observ√©e sur l'avant-bras droit, d√©mangeaisons intenses. Possible r√©action allergique.",
              "metadata": { "patient_id": "P002", "source_type": "Observation infirmi√®re" }
            }
            ```
    *   **Observations attendues :**
        *   Le client gRPC re√ßoit une r√©ponse de succ√®s (ex: `{ "status": "SUCCESS" }`).
        *   Logs du `Content Receiver Service` indiquant la r√©ception et la production du message vers Kafka.
    *   **Capture d'√©cran sugg√©r√©e (grpc_client_request.png) :**
        *   Votre client gRPC (BloomRPC, Postman) montrant l'appel et la r√©ponse.
        ```
        <!-- Ins√©rez ici une capture d'√©cran du client gRPC montrant l'appel et la r√©ponse -->
        ```

2.  **√âtape 2: Suite du Traitement**
    *   **Action :** Les √©tapes suivantes (v√©rification sur Kafka, traitement par Classifier, stockage, consultation) sont identiques aux √©tapes 2 √† 8 du Flux 1.
    *   **Observations attendues :** Similaires au Flux 1, mais avec les donn√©es du message gRPC.
    *   **Captures d'√©cran sugg√©r√©es :** Vous pouvez r√©utiliser les types de captures d'√©cran du Flux 1, mais avec les donn√©es issues de la soumission gRPC.

---

### Notes sur les Captures d'√©cran

*   Remplacez les commentaires `<!-- Ins√©rez ici une capture d'√©cran ... -->` par de vraies images.
*   Utilisez des noms de fichiers descriptifs pour vos captures d'√©cran.
*   Assurez-vous que les captures sont claires et mettent en √©vidence l'information pertinente.
*   Vous pouvez les int√©grer directement dans le README si votre plateforme le supporte (ex: GitHub) en utilisant la syntaxe Markdown : `![Texte alternatif](chemin/vers/votre_image.png)`

# ... (Section Am√©liorations Possibles et reste du README) ...
## üõ†Ô∏è Technologies Utilis√©es

| Composant          | Technologie                           |
| ------------------ | ------------------------------------- |
| Langage principal  | Node.js                               |
| Web/API Framework  | Express.js                            |
| API GraphQL        | Apollo Server                         |
| Base de donn√©es    | MongoDB + Mongoose                    |
| Bus de messages    | Apache Kafka + kafkajs                |
| IA / NLP           | LM Studio (local, mod√®le open-source) |
| gRPC Communication | `@grpc/grpc-js`, `@grpc/proto-loader` |
| HTTP Client        | Axios                                 |

---

## ‚öôÔ∏è Pr√©requis

* Node.js v16+
* Apache Kafka & Zookeeper (localhost:9092)
* MongoDB (localhost:27017)
* LM Studio (localhost:1234)

  * Mod√®le charg√© (ex : `deepseek-r1-distill-qwen-7b`)
  * Serveur API actif (ex : `/v1/chat/completions`)

---

## üì¶ Installation

Clonez le d√©p√¥t :

```bash
git clone https://github.com/votre-utilisateur/medical-pipeline.git
cd medical-pipeline
```

Installez les d√©pendances :

```bash
cd api-gateway && npm install
cd ../classifier-service && npm install
cd ../content-receiver && npm install
cd ../storage-service && npm install
cd ../query-service && npm install
```

---

## ‚ñ∂Ô∏è Lancement des Services

**D√©marrage recommand√© dans l‚Äôordre suivant :**

1. Kafka + Zookeeper
2. MongoDB
3. LM Studio (mod√®le charg√©, serveur API actif)
4. Microservices :

```bash
# Dans chaque dossier de service
npm run start
```

---

## üîó Points d'acc√®s API

| Service         | Type         | Port  | URL/Entr√©e                 |
| --------------- | ------------ | ----- | -------------------------- |
| API Gateway     | REST/GraphQL | 8000  | `/api/content`, `/graphql` |
| gRPC Receiver   | gRPC         | 50051 | `ProcessContent()`         |
| Storage Service | REST         | 8001  | `/api/contents`            |
| Query Service   | GraphQL      | 4000  | `/graphql`                 |

---

## üìÅ Structure du Projet (exemple)

```
medical-pipeline/
‚îú‚îÄ‚îÄ api-gateway/
‚îú‚îÄ‚îÄ classifier-service/
‚îú‚îÄ‚îÄ content-receiver/
‚îú‚îÄ‚îÄ query-service/
‚îú‚îÄ‚îÄ storage-service/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ kafka-config.js
‚îî‚îÄ‚îÄ README.md
```

---

## üß© Am√©liorations Possibles

* Ajout de m√©canismes de retry / gestion d‚Äôerreurs (topic `processing-errors`)
* Authentification / s√©curisation des API
* Monitoring (ex: Prometheus + Grafana)
* CI/CD pour d√©ploiement multi-environnement
* Interface Web de visualisation (ex: Next.js + Apollo Client)

---

Souhaitez-vous que je g√©n√®re ce README sous forme de fichier `.md` ?
