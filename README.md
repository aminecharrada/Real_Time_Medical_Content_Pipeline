Absolument ! Voici une proposition de README.md pour votre projet, incluant un sch√©ma Mermaid et une explication d√©taill√©e du fonctionnement.

# Pipeline de Traitement de Contenu M√©dical

Ce projet impl√©mente un pipeline de traitement de contenu m√©dical utilisant une architecture microservices. Il ing√®re des donn√©es textuelles m√©dicales, les classifie, les enrichit √† l'aide d'un mod√®le de langage (LM Studio), les stocke et les expose via des API REST et GraphQL.

## Table des Mati√®res

- [Aper√ßu du Projet](#aper√ßu-du-projet)
- [Fonctionnalit√©s Cl√©s](#fonctionnalit√©s-cl√©s)
- [Architecture](#architecture)
  - [Diagramme d'Architecture (Mermaid)](#diagramme-darchitecture-mermaid)
  - [Flux de Donn√©es D√©taill√©](#flux-de-donn√©es-d√©taill√©)
- [Technologies Utilis√©es](#technologies-utilis√©es)
- [Pr√©requis](#pr√©requis)
- [Installation](#installation)
- [Lancement des Services](#lancement-des-services)
- [Points d'Acc√®s (API Endpoints)](#points-dacc√®s-api-endpoints)
  - [API Gateway (Port 8000)](#api-gateway-port-8000)
  - [Content Receiver Service (Port 50051)](#content-receiver-service-port-50051)
  - [Storage Service (Port 8001)](#storage-service-port-8001)
  - [Query Service (Port 4000)](#query-service-port-4000)
- [Structure du Projet (Exemple)](#structure-du-projet-exemple)
- [Am√©liorations Possibles](#am√©liorations-possibles)

## Aper√ßu du Projet

L'objectif principal est de fournir une plateforme robuste et √©volutive pour traiter et analyser du contenu m√©dical. Le syst√®me est d√©compos√© en plusieurs microservices communiquant de mani√®re asynchrone via Apache Kafka, garantissant d√©couplage et r√©silience.

## Fonctionnalit√©s Cl√©s

*   **Ingestion Multi-canaux :** R√©ception de contenu via API REST et gRPC.
*   **Traitement Asynchrone :** Utilisation de Kafka pour la communication d√©coupl√©e entre les services.
*   **Classification Automatique :** Cat√©gorisation du contenu m√©dical bas√©e sur des mots-cl√©s.
*   **Enrichissement par IA :** Int√©gration avec LM Studio pour obtenir des analyses ou r√©sum√©s de texte.
*   **Persistance des Donn√©es :** Stockage du contenu trait√© dans MongoDB.
*   **Consultation Flexible :** API GraphQL pour des requ√™tes cibl√©es et API REST pour un acc√®s simple.

## Architecture

### Diagramme d'Architecture (Mermaid)

```mermaid
graph TD
    %% Clients externes
    ClientREST[Client Ingestion REST]
    ClientGRPC[Client Ingestion gRPC]
    ClientGraphQL[Client Requ√™te GraphQL]

    %% Points d'entr√©e & requ√™tes
    APIGW["API Gateway<br>(localhost:8000)<br>Express, Apollo Server"]
    GRPCReceiver["Content Receiver<br>(localhost:50051)<br>gRPC Server"]
    GraphQLQuery["Query Service<br>(localhost:4000)<br>Apollo Server"]

    %% Kafka Topics
    TopicIncoming["Topic:<br>incoming-medical-content"]
    TopicClassified["Topic:<br>classified-content"]

    %% Traitement
    Classifier["Classifier Service<br>Kafka Consumer/Producer"]
    LMStudio[(LM Studio Externe)]

    %% Stockage
    Storage["Storage Service<br>(localhost:8001)<br>Express, Mongoose, Kafka Consumer"]
    Mongo[(MongoDB)]

    %% Ingestion Flow
    ClientREST -->|"POST /api/content"| APIGW
    APIGW -->|"Produit msg"| TopicIncoming

    ClientGRPC -->|"ProcessContent"| GRPCReceiver
    GRPCReceiver -->|"Produit msg"| TopicIncoming

    %% Traitement Flow
    Classifier -->|"Consomme"| TopicIncoming
    Classifier -->|"Appel HTTP POST"| LMStudio
    LMStudio -->|"Texte enrichi"| Classifier
    Classifier -->|"Produit msg classifi√©"| TopicClassified

    %% Stockage Flow
    Storage -->|"Consomme"| TopicClassified
    Storage -->|"Stocke/Lit"| Mongo

    %% Requ√™te Flow
    ClientGraphQL -->|"Requ√™te GraphQL /graphql"| APIGW
    APIGW -->|"Resolver appelle GET /api/contents"| Storage

    ClientGraphQL -->|"Requ√™te GraphQL /graphql"| GraphQLQuery
    GraphQLQuery -->|"Resolver appelle GET /api/contents"| Storage

    %% Classes
    classDef service fill:#D6EAF8,stroke:#2980B9,stroke-width:2px,color:#000;
    class APIGW,GRPCReceiver,GraphQLQuery,Classifier,Storage service;

    classDef topic fill:#FDEBD0,stroke:#E67E22,stroke-width:2px,color:#000;
    class TopicIncoming,TopicClassified topic;

    classDef db fill:#D5F5E3,stroke:#27AE60,stroke-width:2px,color:#000;
    class Mongo db;

    classDef external fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000;
    class LMStudio external;

    classDef client fill:#EAEDED,stroke:#7F8C8D,stroke-width:2px,color:#000;
    class ClientREST,ClientGRPC,ClientGraphQL client;

```

Flux de Donn√©es D√©taill√©

Ingestion du Contenu :

Via API Gateway (REST) : Un client envoie une requ√™te POST √† /api/content de l'API Gateway. Le corps de la requ√™te contient { "text": "...", "metadata": {...} }. L'API Gateway publie ensuite ce message sur le topic Kafka incoming-medical-content.

Via Content Receiver (gRPC) : Un client gRPC appelle la m√©thode ProcessContent du Content Receiver Service avec les donn√©es du contenu. Ce service publie √©galement le message sur le topic Kafka incoming-medical-content.

Classification et Enrichissement :

Le Classifier Service est un consommateur du topic incoming-medical-content.

Pour chaque message re√ßu, il effectue une classification basique en cherchant des mots-cl√©s m√©dicaux pr√©d√©finis dans le texte.

Ensuite, il envoie le texte √† une instance locale de LM Studio (via un appel HTTP POST √† http://localhost:1234/v1/chat/completions) pour obtenir un enrichissement (analyse, r√©sum√©, etc.).

Le Classifier Service combine les donn√©es originales, la cat√©gorie et l'enrichissement de LM Studio, puis publie ce nouveau message sur le topic Kafka classified-content.

Stockage :

Le Storage Service est un consommateur du topic classified-content.

Lorsqu'il re√ßoit un message, il le persiste dans une base de donn√©es MongoDB (collection contents).

Ce service expose √©galement une API REST (GET /api/contents) pour permettre la r√©cup√©ration de toutes les donn√©es stock√©es.

Consultation du Contenu :

Via API Gateway (GraphQL) : L'API Gateway expose un endpoint GraphQL (/graphql). Les resolvers pour les requ√™tes de contenu (ex: query { contents { ... } }) interrogent l'API REST (GET /api/contents) du Storage Service pour r√©cup√©rer les donn√©es.

Via Query Service (GraphQL) : Un service GraphQL d√©di√© (Query Service sur le port 4000) offre √©galement des capacit√©s de requ√™tes avanc√©es (ex: contentsByCategory). Ses resolvers interrogent aussi l'API REST du Storage Service.

Technologies Utilis√©es

Langage/Runtime : Node.js

Frameworks Web/API : Express.js

Communication Asynchrone : Apache Kafka (avec la librairie kafkajs)

Base de Donn√©es : MongoDB (avec l'ODM mongoose)

API GraphQL : Apollo Server (apollo-server-express, apollo-server, graphql-type-json)

Communication RPC : gRPC (@grpc/grpc-js, @grpc/proto-loader)

Client HTTP : Axios (pour appeler LM Studio)

Mod√®le de Langage : LM Studio (ex√©cut√© localement comme un service externe)

Pr√©requis

Avant de commencer, assurez-vous d'avoir install√© :

Node.js (v16 ou sup√©rieur recommand√©)

npm ou yarn

Apache Kafka (et Zookeeper) fonctionnant localement (par d√©faut sur localhost:9092)

MongoDB fonctionnant localement (par d√©faut sur mongodb://localhost:27017/medical)

LM Studio install√© et ex√©cut√© localement, avec un mod√®le compatible charg√© (par exemple, deepseek-r1-distill-qwen-7b mentionn√© dans le code) et le serveur API d√©marr√© (par d√©faut sur http://localhost:1234).

Installation

Clonez ce d√©p√¥t :

git clone <votre-url-de-repo>
cd <nom-du-repo>
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Installez les d√©pendances pour chaque service. Si vous avez une structure de monorepo avec des package.json s√©par√©s dans chaque dossier de service (ex: api-gateway/, classifier-service/, etc.) :

cd api-gateway
npm install
cd ../classifier-service
npm install
# R√©p√©tez pour chaque service (content-receiver, storage-service, query-service)
cd ..
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Si vous avez un package.json unique √† la racine et que les imports relatifs sont corrects, un npm install √† la racine pourrait suffire.

Configuration Kafka :
Assurez-vous que les topics Kafka sont cr√©√©s (ou que Kafka est configur√© pour les cr√©er automatiquement) :

incoming-medical-content

classified-content

processing-errors (optionnel, mais une bonne pratique)

Le fichier config/kafka-config.js centralise la configuration du client Kafka.

Fichier .proto :
Le fichier content-receiver/content_receiver.proto est n√©cessaire pour le service gRPC. Assurez-vous qu'il est pr√©sent et correct. Exemple :

// content-receiver/content_receiver.proto
syntax = "proto3";

package contentreceiver;

import "google/protobuf/struct.proto";

service ContentReceiver {
  rpc ProcessContent (ProcessContentRequest) returns (ProcessContentResponse);
}

message ProcessContentRequest {
  string text = 1;
  google.protobuf.Struct metadata = 2; // Ou string json_metadata = 2;
}

message ProcessContentResponse {
  string status = 1;
  optional string message = 2;
}
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Protobuf
IGNORE_WHEN_COPYING_END
Lancement des Services

Il est important de d√©marrer les services dans le bon ordre en raison des d√©pendances.

D√©pendances Externes :

D√©marrez Zookeeper et Apache Kafka.

D√©marrez MongoDB.

D√©marrez LM Studio et assurez-vous que le serveur API est actif et que le mod√®le deepseek-r1-distill-qwen-7b (ou celui configur√©) est charg√©.

Microservices de l'Application :
Ouvrez un terminal pour chaque service et naviguez vers son r√©pertoire.

Content Receiver Service :

cd content-receiver # (ou le chemin appropri√©)
node server.js # (ou le nom de votre fichier principal)
# Devrait afficher: gRPC server running on http://localhost:50051
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Classifier Service :

cd classifier-service # (ou le chemin appropri√©)
node classifier.js # (ou le nom de votre fichier principal)
# Devrait afficher des logs de connexion Kafka et d'attente de messages
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Storage Service :

cd storage-service # (ou le chemin appropri√©)
node storage.js # (ou le nom de votre fichier principal)
# Devrait afficher: Connected to MongoDB
# Devrait afficher: Storage Service running on http://localhost:8001
# Et des logs de connexion Kafka
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Query Service :

cd query-service # (ou le chemin appropri√©)
node server.js # (ou le nom de votre fichier principal)
# Devrait afficher: üöÄ Query Service ready at http://localhost:4000
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

API Gateway :

cd api-gateway # (ou le chemin appropri√©)
node server.js # (ou le nom de votre fichier principal)
# Devrait afficher: Kafka producer connected
# Devrait afficher: API Gateway running on http://localhost:8000
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END
Points d'Acc√®s (API Endpoints)
API Gateway (Port 8000)

REST Endpoint : POST /api/content

R√¥le : Soumettre un nouveau contenu m√©dical.

Request Body (JSON) :

{
  "text": "Le patient se plaint de douleurs thoraciques et d'un rythme cardiaque irr√©gulier.",
  "metadata": { "patient_id": "12345", "source_system": "EHR-XYZ" }
}
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Json
IGNORE_WHEN_COPYING_END

R√©ponse (JSON) :

{ "message": "Content received", "status": "SUCCESS" }
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Json
IGNORE_WHEN_COPYING_END

Exemple curl :

curl -X POST -H "Content-Type: application/json" \
-d '{ "text": "Le patient se plaint de douleurs thoraciques et d''un rythme cardiaque irr√©gulier.", "metadata": { "patient_id": "12345" } }' \
http://localhost:8000/api/content
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

GraphQL Endpoint : POST /graphql (ou via le Playground Apollo si activ√©)

R√¥le : Interroger le contenu m√©dical stock√©.

Exemple de requ√™te :

query GetContents {
  contents {
    id
    text
    category
    timestamp
    metadata
  }
}
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Graphql
IGNORE_WHEN_COPYING_END
Content Receiver Service (Port 50051)

gRPC Service : contentreceiver.ContentReceiver

M√©thode : ProcessContent

R√¥le : Soumettre un nouveau contenu m√©dical via gRPC.

Requ√™te (bas√©e sur content_receiver.proto) : ProcessContentRequest { text: "...", metadata: Struct{...} }

R√©ponse : ProcessContentResponse { status: "SUCCESS" }

Note : N√©cessite un client gRPC pour interagir.

Storage Service (Port 8001)

REST Endpoint : GET /api/contents

R√¥le : R√©cup√©rer tous les contenus m√©dicaux stock√©s.

R√©ponse (JSON) : Tableau d'objets MedicalContent.

[
  {
    "id": "60c72b2f9b1e8a3f7c8d4f5e",
    "text": "Le patient se plaint de douleurs thoraciques...",
    "category": "cardiology",
    "timestamp": "2023-01-15T10:30:00.000Z",
    "metadata": { "patient_id": "12345" },
    "lm_enhancement": "Analyse: Douleurs thoraciques sugg√©rant un potentiel probl√®me cardiaque."
  },
  // ... autres contenus
]
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Json
IGNORE_WHEN_COPYING_END

Exemple curl :

curl http://localhost:8001/api/contents
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END
Query Service (Port 4000)

GraphQL Endpoint : POST /graphql (ou via le Playground Apollo si activ√©)

R√¥le : Interroger le contenu m√©dical stock√©, avec des options de filtrage.

Exemple de requ√™te (tous les contenus) :

query GetAllContents {
  contents {
    id
    text
    category
    timestamp
    metadata
    lm_enhancement
  }
}
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Graphql
IGNORE_WHEN_COPYING_END

Exemple de requ√™te (filtr√©e par cat√©gorie) :

query GetCardiologyContents {
  contentsByCategory(category: "cardiology") {
    id
    text
    category
    lm_enhancement
  }
}
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Graphql
IGNORE_WHEN_COPYING_END
Structure du Projet (Exemple)
.
‚îú‚îÄ‚îÄ api-gateway/
‚îÇ   ‚îú‚îÄ‚îÄ server.js
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ classifier-service/
‚îÇ   ‚îú‚îÄ‚îÄ classifier.js
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ content-receiver/
‚îÇ   ‚îú‚îÄ‚îÄ content_receiver.proto
‚îÇ   ‚îú‚îÄ‚îÄ server.js
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ query-service/
‚îÇ   ‚îú‚îÄ‚îÄ server.js
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ storage-service/
‚îÇ   ‚îú‚îÄ‚îÄ storage.js
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ kafka-config.js
‚îî‚îÄ‚îÄ README.md
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END

(Cette structure est une supposition bas√©e sur les imports. Adaptez-la √† votre structure r√©elle.)

Am√©liorations Possibles

Gestion d'Erreurs Robuste : Mettre en place des Dead Letter Queues (DLQ) dans Kafka pour les messages qui ne peuvent pas √™tre trait√©s.

Authentification et Autorisation : S√©curiser les endpoints.

Monitoring et Logging : Int√©grer des outils comme Prometheus/Grafana et ELK Stack.

Tests : Ajouter des tests unitaires, d'int√©gration et de bout-en-bout.

Containerisation : Utiliser Docker et Docker Compose pour faciliter le d√©ploiement et le d√©veloppement.

CI/CD : Mettre en place un pipeline d'int√©gration et de d√©ploiement continus.

Configuration Centralis√©e : Utiliser des variables d'environnement ou un service de configuration.

Affinement de LM Studio : Optimiser les prompts et les param√®tres de LM Studio pour de meilleurs r√©sultats.

