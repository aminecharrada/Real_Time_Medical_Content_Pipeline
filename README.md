TrÃ¨s bienâ€¯! Voici une version **recrÃ©Ã©e et structurÃ©e proprement** de votre `README.md`, incluant un schÃ©ma Mermaid, des sections claires et professionnelles, tout en conservant lâ€™esprit de votre message original.

---

# ğŸ©º Pipeline de Traitement de Contenu MÃ©dical

Ce projet met en Å“uvre un pipeline distribuÃ© pour ingÃ©rer, enrichir, stocker et interroger du contenu mÃ©dical. Il s'appuie sur une architecture microservices, Kafka pour la communication asynchrone, MongoDB pour le stockage et LM Studio pour lâ€™enrichissement par IA.

---

## ğŸ“‘ Table des MatiÃ¨res

* [ğŸš€ AperÃ§u du Projet](#-aperÃ§u-du-projet)
* [âœ¨ FonctionnalitÃ©s ClÃ©s](#-fonctionnalitÃ©s-clÃ©s)
* [ğŸ§± Architecture Globale](#-architecture-globale)

  * [ğŸ“Š Diagramme Mermaid](#-diagramme-mermaid)
  * [ğŸ” DÃ©tail du Flux de DonnÃ©es](#-dÃ©tail-du-flux-de-donnÃ©es)
* [ğŸ› ï¸ Technologies UtilisÃ©es](#-technologies-utilisÃ©es)
* [âš™ï¸ PrÃ©requis](#-prÃ©requis)
* [ğŸ“¦ Installation](#-installation)
* [â–¶ï¸ Lancement des Services](#ï¸-lancement-des-services)
* [ğŸ”— Points d'AccÃ¨s API](#-points-daccÃ¨s-api)
* [ğŸ“ Structure du Projet](#-structure-du-projet)
* [ğŸ§© AmÃ©liorations Possibles](#-amÃ©liorations-possibles)

---

## ğŸš€ AperÃ§u du Projet

L'objectif est de construire un systÃ¨me **scalable** et **modulaire** pour traiter du texte mÃ©dical. Les messages sont transmis entre microservices via **Apache Kafka**, enrichis par un modÃ¨le de langage (via **LM Studio**), stockÃ©s dans **MongoDB** et consultables via des API **REST** et **GraphQL**.

---

## âœ¨ FonctionnalitÃ©s ClÃ©s

âœ… Ingestion via API REST ou gRPC

âœ… Traitement asynchrone et dÃ©couplÃ© via Kafka

âœ… Classification automatique par mots-clÃ©s

âœ… Enrichissement intelligent via LM Studio

âœ… Stockage durable dans MongoDB

âœ… AccÃ¨s flexible via GraphQL ou REST


---

## ğŸ§± Architecture Globale

### ğŸ“Š Diagramme Mermaid

```mermaid
graph TD
    %% Clients externes
    ClientREST[Client Ingestion REST]
    ClientGRPC[Client Ingestion gRPC]
    ClientGraphQL[Client RequÃªte GraphQL]

    %% Points d'entrÃ©e & requÃªtes
    APIGW["API Gateway<br>(localhost:8000)<br>Express, Apollo Server"]
    GRPCReceiver["Content Receiver<br>(localhost:50051)<br>gRPC Server"]
    GraphQLQuery["Query Service<br>(localhost:4000)<br>Apollo Server"]

    %% Kafka Topics
    TopicIncoming["Topic:<br>incoming-medical-content"]
    TopicClassified["Topic:<br>classified-content"]

    %% Traitement
    Classifier["Classifier Service<br>Kafka Consumer/Producer"]
    LMStudio[(LM Studio Externe)]

    %% Stockage
    Storage["Storage Service<br>(localhost:8001)<br>Express, Mongoose, Kafka Consumer"]
    Mongo[(MongoDB)]

    %% Ingestion Flow
    ClientREST -->|"POST /api/content"| APIGW
    APIGW -->|"Produit msg"| TopicIncoming

    ClientGRPC -->|"ProcessContent"| GRPCReceiver
    GRPCReceiver -->|"Produit msg"| TopicIncoming

    %% Traitement Flow
    Classifier -->|"Consomme"| TopicIncoming
    Classifier -->|"Appel HTTP POST"| LMStudio
    LMStudio -->|"Texte enrichi"| Classifier
    Classifier -->|"Produit msg classifiÃ©"| TopicClassified

    %% Stockage Flow
    Storage -->|"Consomme"| TopicClassified
    Storage -->|"Stocke/Lit"| Mongo

    %% RequÃªte Flow
    ClientGraphQL -->|"RequÃªte GraphQL /graphql"| APIGW
    APIGW -->|"Resolver appelle GET /api/contents"| Storage

    ClientGraphQL -->|"RequÃªte GraphQL /graphql"| GraphQLQuery
    GraphQLQuery -->|"Resolver appelle GET /api/contents"| Storage

    %% Classes
    classDef service fill:#D6EAF8,stroke:#2980B9,stroke-width:2px,color:#000;
    class APIGW,GRPCReceiver,GraphQLQuery,Classifier,Storage service;

    classDef topic fill:#FDEBD0,stroke:#E67E22,stroke-width:2px,color:#000;
    class TopicIncoming,TopicClassified topic;

    classDef db fill:#D5F5E3,stroke:#27AE60,stroke-width:2px,color:#000;
    class Mongo db;

    classDef external fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000;
    class LMStudio external;

    classDef client fill:#EAEDED,stroke:#7F8C8D,stroke-width:2px,color:#000;
    class ClientREST,ClientGRPC,ClientGraphQL client;

```

---

## ğŸ” DÃ©tail du Flux de DonnÃ©es

1. **Ingestion**

   * Un client envoie une requÃªte `POST /api/content` (REST) ou appelle `ProcessContent` (gRPC).
   * Le message est publiÃ© sur `incoming-medical-content`.

2. **Classification & Enrichissement**

   * Le `Classifier Service` consomme les messages.
   * Classification simple basÃ©e sur mots-clÃ©s.
   * Envoi du texte Ã  **LM Studio** pour rÃ©sumÃ©/analyse.
   * RÃ©sultat enrichi publiÃ© sur `classified-content`.

3. **Stockage**

   * Le `Storage Service` consomme le topic `classified-content`.
   * Les donnÃ©es sont enregistrÃ©es dans **MongoDB**.
   * Une API REST (`GET /api/contents`) permet la rÃ©cupÃ©ration.

4. **Consultation**

   * Via API Gateway (GraphQL) ou `Query Service`, les clients peuvent interroger les contenus.
   * Les resolvers appellent lâ€™API REST du `Storage Service`.

---

## ğŸ› ï¸ Technologies UtilisÃ©es

| Composant          | Technologie                           |
| ------------------ | ------------------------------------- |
| Langage principal  | Node.js                               |
| Web/API Framework  | Express.js                            |
| API GraphQL        | Apollo Server                         |
| Base de donnÃ©es    | MongoDB + Mongoose                    |
| Bus de messages    | Apache Kafka + kafkajs                |
| IA / NLP           | LM Studio (local, modÃ¨le open-source) |
| gRPC Communication | `@grpc/grpc-js`, `@grpc/proto-loader` |
| HTTP Client        | Axios                                 |

---

## âš™ï¸ PrÃ©requis

* Node.js v16+
* Apache Kafka & Zookeeper (localhost:9092)
* MongoDB (localhost:27017)
* LM Studio (localhost:1234)

  * ModÃ¨le chargÃ© (ex : `deepseek-r1-distill-qwen-7b`)
  * Serveur API actif (ex : `/v1/chat/completions`)

---

## ğŸ“¦ Installation

Clonez le dÃ©pÃ´t :

```bash
git clone https://github.com/votre-utilisateur/medical-pipeline.git
cd medical-pipeline
```

Installez les dÃ©pendances :

```bash
cd api-gateway && npm install
cd ../classifier-service && npm install
cd ../content-receiver && npm install
cd ../storage-service && npm install
cd ../query-service && npm install
```

---

## â–¶ï¸ Lancement des Services

**DÃ©marrage recommandÃ© dans lâ€™ordre suivant :**

1. Kafka + Zookeeper
2. MongoDB
3. LM Studio (modÃ¨le chargÃ©, serveur API actif)
4. Microservices :

```bash
# Dans chaque dossier de service
npm run start
```

---

## ğŸ”— Points d'accÃ¨s API

| Service         | Type         | Port  | URL/EntrÃ©e                 |
| --------------- | ------------ | ----- | -------------------------- |
| API Gateway     | REST/GraphQL | 8000  | `/api/content`, `/graphql` |
| gRPC Receiver   | gRPC         | 50051 | `ProcessContent()`         |
| Storage Service | REST         | 8001  | `/api/contents`            |
| Query Service   | GraphQL      | 4000  | `/graphql`                 |

---

## ğŸ“ Structure du Projet (exemple)

```
medical-pipeline/
â”œâ”€â”€ api-gateway/
â”œâ”€â”€ classifier-service/
â”œâ”€â”€ content-receiver/
â”œâ”€â”€ query-service/
â”œâ”€â”€ storage-service/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ kafka-config.js
â””â”€â”€ README.md
```

---

## ğŸ§© AmÃ©liorations Possibles

* Ajout de mÃ©canismes de retry / gestion dâ€™erreurs (topic `processing-errors`)
* Authentification / sÃ©curisation des API
* Monitoring (ex: Prometheus + Grafana)
* CI/CD pour dÃ©ploiement multi-environnement
* Interface Web de visualisation (ex: Next.js + Apollo Client)

---

Souhaitez-vous que je gÃ©nÃ¨re ce README sous forme de fichier `.md` ?
